public class IoTest {

    /**
     * i/o多路复用机制:一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求
     * 常见的三个多路复用机制有select/poll/epoll
     * select/poll:
     * 1：将已连接的socket全部放到文件描述集合（BitMap）里 然后调用 select 函数将文件描述符集合拷贝到内核里，（用户线程从用户态转换成内核态完成了一次上下文切换）
     * 文件描述符集合其实是一个bitmap 他的下标是文件描述符fd,下标对应的值 1 表示有读写事件 0表示没有
     * 2：用户线程开始进入阻塞状态 内核使用轮询的方式来便利文件描述集合 查看fd对应的socket接受缓冲区中是否有数据到来，如果有设置为1 没有设置为0
     * 3：内核便利一遍之后，如果发现有有些socket 有数据到来，则会将集合从内核空间拷贝到用户空间
     * 4：用户线程再次遍历文件描述集合 找到其中有数据到来的socket 进行处理，
     * 5：由于内核在遍历过程中已经修改了fd数据，用户线程遍历结束之后，就需要重置fd 并调用select 让内核重新开始轮询
     *
     * 对于 select 这种方式，需要进行 2 次「遍历」文件描述符集合，一次是在内核态里，一个次是在用户态里，而且还会发生 2 次「拷贝」文件描述符集合，
     * 先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，
     * 在 Linux 系统中，由内核中的 FD_SETSIZE 限制（Linux内核对集合的大小做了限制）， 默认最大值为 1024，只能监听 0~1023 的文件描述符。
     *
     * poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。
     * 但是 poll 和 select 并没有太大的本质区别，都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，
     * 而且也需要在用户态与内核态之间拷贝文件描述符集合，这种方式随着并发数上来，性能的损耗会呈指数级增长。
     *
     * epoll： 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，
     * 不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。epoll 使用事件驱动的机制，
     * 内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ）
     * ，大大提高了检测的效率。因而，epoll 被称为解决 C10K 问题的利器。
     *
     * epoll中的三个API函数
     * int epoll_create(int size);（创建epoll实例，通过一棵红黑树管理待检测集合）
     * int epoll_ctl（管理红黑树上的文件描述符(添加、修改、删除)）
     * int epoll_wait（当它被调用时它观察 epoll->rdllist 链表里有没有数据即可。有数据就返回，没有数据就创建一个等待队列项，
     *                 将其添加到 epoll 的等待队列上，然后把自己阻塞掉（进入睡眠状态）就完事。）
     *
     * epoll实例结构
     * struct eventpoll {
     * wq： 等待队列链表。软中断数据就绪的时候会通过 wq 来找到阻塞在 epoll 对象上的用户线程。
     * rbr： 一棵红黑树。为了支持对海量连接的高效查找、插入和删除，epoll 内部使用了一棵红黑树。通过这棵树来管理用户进程下添加进来的所有 socket 连接。
     * rdllist： 就绪的描述符的链表。当有的连接就绪的时候，内核会把就绪的连接放到 rdllist链表里。这样应用进程只需要判断链表就能找出就绪描述符，而不用去遍历整棵树。
     * }
     * 假如我们与客户链接的socket创建好了，先通过epoll_create创建一个epoll实例 每有一个客户端socket连接注册时，内核会：
     * 1：为这个socket分配一个结点epitem
     * 2：添加等待事件到等待队列wq中，并注册其回调函数为ep_poll_callback，
     * 3：将epitem加入到epoll对象的红黑树中
     *
     * ①当数据到来的时候，先将接收的数据放入socket的接收队列上。
     * ②当 socket 上数据就绪时候，唤醒在 socket上等待的用户进程，内核会找到 epoll_ctl 添加 socket 时在其上设置的回调函数 ep_poll_callback。
     * ③接着软中断就会调用这个回调函数ep_poll_callback，它会根据等待任务队列项上的额外的 base 指针可以找到 epitem， 进而也可以找到 epoll对象。
     * ④首先它做的第一件事就是把自己的 epitem 添加到 epoll 的就绪队列中。
     * ⑤接着它又会查看 epoll对象上的等待队列里是否有等待项（epoll_wait 执行的时候会设置，上面提到了）。
     * ⑥如果没有，执行软中断的事情就做完了。如果有等待项，那就查找到等待项里设置的回调函数default_wake_function。
     * ⑦在default_wake_function 中找到等待队列项里的进程描述符，然后唤醒之。
     *
     */

    /**
     *      * 一个线程需要的资源，通常为cpu 内存 i/o cpu负责运行  内存负责存储 i/o负责和外部交互
     *      *  客户端和请求端进行i/o的过程是
     *      *  1：服务端开一个口子socket()
     *      *  2：bind() 绑定ip地址和端口号（一台机器可能有多个网卡，即多个ip地址）
     *      *  3:listen() 监听socket()有没有被调用
     *      *  4:accept（） 如果客户端请求到内核，则从内核拿到客户端的连接 没有 则等待客户端连接的到来
     *      *  5：客户端发起请求 也开一个口子 socket()
     *      *  6：创建connect()函数发起连接主要指明ip地址和端口号
     *      *  7：客户端和服务器开始三次握手 服务器内核为每个socket维护了两个队列 一个是tcp半连接队列 一个是tcp全连接队列
     *      *  如果全连接队列不为空，就会从里面拿出一个socket使用，后续的连接处理也都会用这个socket
     *      *  8:客户端和服务端进行read 和 write 操作。
     *      *
     *      *  传统方法使用的多进程 ：客户端的请求过来accept()函数就返回一个全连接队列socket 然后fork()函数创建一个子进程，实际上就是把父进程的所有相关东西
     *      *  (文件描述符，内存地址空间，程序计数器，执行的代码等)复制给子进程，子进程这两个进程刚复制完的时候，几乎一摸一样
     *      *  。不过，会根据返回值来区分是父进程还是子进程，如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。
     *      *  每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣。
     *      *  服务端开100个子进程还可以，如果一时间有了10万个连接一块（C10K问题），则服务器需要开10万个口子，服务器必定凉凉
     *      *
     *      *  接下来进入多线程模型 ：当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，为了减少线程创建和销毁的开销，使用线程池，
     *      *  由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接 Socket 进程处理
     *      *  需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。
     *      *  上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，
     *      *  相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。
     *      *
     *      *
     *      *  因此引入了i/o多路复用机制：一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，
     *      *  把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。
     */



}
